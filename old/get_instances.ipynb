{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8565b05e-f7a0-4013-8506-5cbf7d2685c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from to_json import to_json\n",
    "from gdsfactory.pdk import get_layer_stack\n",
    "import gdsfactory as gf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668f069c-5422-44ad-8ba3-0a32611e0115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sky130 import LAYER_STACK\n",
    "\n",
    "c = gf.read.import_gds(\"example_sky130.gds\")\n",
    "layer_stack = LAYER_STACK\n",
    "name = \"sky130\"\n",
    "# j = to_json(c, layer_stack=layer_stack, return_json=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b3d9ec-3cb6-4793-9433-70a454afbca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import cast\n",
    "from sky130 import LAYER_STACK\n",
    "\n",
    "from kfactory import LayerEnum\n",
    "\n",
    "from gdsfactory.component import Component\n",
    "from gdsfactory.technology import DerivedLayer, LayerStack, LayerViews, LogicalLayer\n",
    "from gdsfactory.typings import LayerSpecs\n",
    "from gdsfactory.pdk import (\n",
    "    get_active_pdk,\n",
    "    get_layer,\n",
    "    get_layer_stack,\n",
    "    get_layer_views,\n",
    ")\n",
    "\n",
    "import gdsfactory as gf\n",
    "\n",
    "\n",
    "def to_poly(\n",
    "    component: Component,\n",
    "    layer_views: LayerViews | None = None,\n",
    "    layer_stack: LayerStack | None = None,\n",
    "    exclude_layers: LayerSpecs | None = None,\n",
    "    return_json: bool = True,\n",
    "):\n",
    "    \"\"\"Return optimzed json.\n",
    "\n",
    "    Args:\n",
    "        component: to extrude in 3D.\n",
    "        layer_views: layer colors from Klayout Layer Properties file.\n",
    "            Defaults to active PDK.layer_views.\n",
    "        layer_stack: contains thickness and zmin for each layer.\n",
    "            Defaults to active PDK.layer_stack.\n",
    "        exclude_layers: list of layer index to exclude.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    layer_views = layer_views or get_layer_views()\n",
    "    layer_stack = layer_stack or get_layer_stack()\n",
    "\n",
    "    exclude_layers = exclude_layers or ()\n",
    "    exclude_layers = [get_layer(layer) for layer in exclude_layers]\n",
    "\n",
    "    component_with_booleans = layer_stack.get_component_with_derived_layers(component)\n",
    "    polygons_per_layer = component_with_booleans.get_polygons_points(merge=True)\n",
    "    has_polygons = False\n",
    "\n",
    "    top_name = \"GDS\"\n",
    "\n",
    "    polygons = {}\n",
    "    \n",
    "    for level in layer_stack.layers.values():\n",
    "        layer = level.layer\n",
    "\n",
    "        if isinstance(layer, LogicalLayer):\n",
    "            assert isinstance(layer.layer, tuple | LayerEnum)\n",
    "            layer_tuple = cast(tuple[int, int], tuple(layer.layer))\n",
    "        elif isinstance(layer, DerivedLayer):\n",
    "            assert level.derived_layer is not None\n",
    "            assert isinstance(level.derived_layer.layer, tuple | LayerEnum)\n",
    "            layer_tuple = cast(tuple[int, int], tuple(level.derived_layer.layer))\n",
    "        else:\n",
    "            raise ValueError(f\"Layer {layer!r} is not a DerivedLayer or LogicalLayer\")\n",
    "\n",
    "        layer_index = int(get_layer(layer_tuple))\n",
    "\n",
    "        if layer_index in exclude_layers:\n",
    "            continue\n",
    "\n",
    "        if layer_index not in polygons_per_layer:\n",
    "            continue\n",
    "\n",
    "        zmin = level.zmin\n",
    "        height = level.thickness\n",
    "        layer_view = layer_views.get_from_tuple(layer_tuple)\n",
    "\n",
    "        assert layer_view.fill_color is not None\n",
    "        if zmin is not None and layer_view.visible:\n",
    "            has_polygons = True\n",
    "            polygons[layer_view.name] = polygons_per_layer[layer_index]\n",
    "\n",
    "    if not has_polygons:\n",
    "        raise ValueError(\n",
    "            f\"{component.name!r} does not have polygons defined in the \"\n",
    "            f\"layer_stack or layer_views for the active Pdk {get_active_pdk().name!r}\"\n",
    "        )\n",
    "\n",
    "    return polygons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc9092e-afb5-41cc-8a0e-903f65e3e705",
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons = to_poly(c, layer_stack=LAYER_STACK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c0e7b1-5d6f-467f-9734-8f7bf0c323fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dba732-8b2d-4601-892e-faa9d3f33917",
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon = polygons['polydrawing_m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b59bf54-ed21-40e0-a811-c62a1f49a39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def plot_polygons(polygons, width=1300, height=1300):\n",
    "    \"\"\"\n",
    "    Plot all original polygons in one interactive Plotly figure.\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "    # colors = [f\"rgba({100+30*i%155},{100+60*i%155},{200-30*i%155},1)\" for i in range(len(polygons))]\n",
    "    colors = [f\"rgba(0.3, 0.3, 0.3, 1)\" for i in range(len(polygons))]\n",
    "    fill_colors = [f\"rgba(0.3, 0.3, 0.3, 0.2)\" for i in range(len(polygons))]\n",
    "    for i, poly in enumerate(polygons):\n",
    "        poly = np.array(poly)\n",
    "        poly_closed = np.vstack([poly, poly[0]])\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=poly_closed[:, 0], y=poly_closed[:, 1],\n",
    "            mode='lines',\n",
    "            name=f\"Original {i}\",\n",
    "            line=dict(width=2, color=colors[i]),\n",
    "            fill='toself',\n",
    "            fillcolor=fill_colors[i]\n",
    "        ))\n",
    "    fig.update_layout(\n",
    "        title=\"All Original Polygons\",\n",
    "        width=width, height=height,\n",
    "        xaxis=dict(scaleanchor=\"y\", scaleratio=1, showgrid=False, zeroline=False),\n",
    "        yaxis=dict(scaleanchor=\"x\", scaleratio=1, showgrid=False, zeroline=False),\n",
    "        showlegend=False,\n",
    "        plot_bgcolor=\"#FFF\",\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "plot_polygons(polygon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bfd762-bc6e-4ac7-859c-ed26dd756c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import njit\n",
    "from collections import defaultdict\n",
    "\n",
    "# Precomputed inverse transformation indices [0-7]\n",
    "INVERSE_MAP = [0, 3, 2, 1, 4, 7, 6, 5]\n",
    "\n",
    "#@njit\n",
    "def translate_to_centroid(points):\n",
    "    centroid = np.mean(points, axis=0)\n",
    "    return points - centroid\n",
    "\n",
    "#@njit\n",
    "def rotate90(points):\n",
    "    rotated = np.empty_like(points)\n",
    "    rotated[:, 0] = points[:, 1]\n",
    "    rotated[:, 1] = -points[:, 0]\n",
    "    return rotated\n",
    "\n",
    "# @njit\n",
    "def rotate180(points):\n",
    "    return -points\n",
    "\n",
    "# @njit\n",
    "def rotate270(points):\n",
    "    rotated = np.empty_like(points)\n",
    "    rotated[:, 0] = -points[:, 1]\n",
    "    rotated[:, 1] = points[:, 0]\n",
    "    return rotated\n",
    "\n",
    "# @njit\n",
    "def reflect(points):\n",
    "    reflected = np.empty_like(points)\n",
    "    reflected[:, 0] = points[:, 0]\n",
    "    reflected[:, 1] = -points[:, 1]\n",
    "    return reflected\n",
    "    \n",
    "#@njit\n",
    "def apply_transformation(points, trans_idx):\n",
    "    if trans_idx == 0: return points\n",
    "    elif trans_idx == 1: return rotate90(points)\n",
    "    elif trans_idx == 2: return rotate180(points)\n",
    "    elif trans_idx == 3: return rotate270(points)\n",
    "    elif trans_idx == 4: return reflect(points)\n",
    "    elif trans_idx == 5: return reflect(rotate90(points))\n",
    "    elif trans_idx == 6: return reflect(rotate180(points))\n",
    "    elif trans_idx == 7: return reflect(rotate270(points))\n",
    "\n",
    "# @njit\n",
    "def normalize_and_sort(points):\n",
    "    \"\"\"Centroid normalization + vertex sorting\"\"\"\n",
    "    centroid = np.mean(points, axis=0)\n",
    "    normalized = points - centroid\n",
    "    order = np.lexsort((normalized[:, 1], normalized[:, 0]))\n",
    "    return normalized[order]\n",
    "\n",
    "def group_congruent_polygons(polygons):\n",
    "    length_groups = defaultdict(list)\n",
    "    \n",
    "    for idx, poly in enumerate(polygons):\n",
    "        poly_array = np.array(poly, dtype=np.float64)\n",
    "        \n",
    "        # Generate all possible canonical forms\n",
    "        min_key = None\n",
    "        for trans_idx in range(8):\n",
    "            transformed = apply_transformation(poly_array, trans_idx)\n",
    "            processed = normalize_and_sort(transformed)\n",
    "            key = tuple(np.round(processed.flatten(), 6))\n",
    "            \n",
    "            if (min_key is None) or (key < min_key):\n",
    "                min_key = key\n",
    "                best_trans = trans_idx\n",
    "                best_processed = processed\n",
    "        \n",
    "        length_groups[(len(poly_array), min_key)].append(\n",
    "            (idx, best_processed, best_trans)\n",
    "        )\n",
    "\n",
    "    # Build groups with reconstruction data\n",
    "    groups = []\n",
    "    for (vertex_count, key), members in length_groups.items():\n",
    "        ref_poly = members[0][1]  # Already normalized and sorted\n",
    "        group_entry = {\n",
    "            'reference': ref_poly.tolist(),\n",
    "            'members': []\n",
    "        }\n",
    "        \n",
    "        for mem_idx, processed_poly, trans_idx in members:\n",
    "            # Calculate inverse transformation to reach original\n",
    "            inverse_trans = INVERSE_MAP[trans_idx]\n",
    "            centroid = np.mean(polygons[mem_idx], axis=0)\n",
    "            \n",
    "            group_entry['members'].append({\n",
    "                'original_index': mem_idx,\n",
    "                'centroid': centroid.tolist(),\n",
    "                'transformation': inverse_trans\n",
    "            })\n",
    "        \n",
    "        groups.append(group_entry)\n",
    "    \n",
    "    return groups\n",
    "\n",
    "def reconstruct_from_group(group, member, apply_transformation):\n",
    "    ref_poly = np.array(group['reference'])\n",
    "    transformed = apply_transformation(ref_poly, member['transformation'])\n",
    "    return transformed + np.array(member['centroid'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5842551f-2098-42b6-acc3-43d1bff18805",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = group_congruent_polygons(polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcecdfd-9c84-4cd8-b8be-4296b2576fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_polygon_groups(groups, apply_transformation, width=900, height=700):\n",
    "    \"\"\"\n",
    "    Plot each congruence group with its canonical reference and all reconstructed members.\n",
    "    \"\"\"\n",
    "    for group_idx, group in enumerate(groups):\n",
    "        fig = go.Figure()\n",
    "        # Reference polygon (canonical form at origin)\n",
    "        ref_poly = np.array(group['reference'])\n",
    "        ref_closed = np.vstack([ref_poly, ref_poly[0]])\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=ref_closed[:, 0], y=ref_closed[:, 1],\n",
    "            mode='lines+markers',\n",
    "            name='Canonical Reference',\n",
    "            line=dict(color='black', width=3, dash='dash'),\n",
    "            marker=dict(color='black', size=6),\n",
    "            fill='toself',\n",
    "            fillcolor='rgba(50,50,50,0.1)'\n",
    "        ))\n",
    "\n",
    "        # All reconstructed group members\n",
    "        # Use a fixed color cycle for consistency\n",
    "        colors = [f\"rgba({100+30*i%155},{100+60*i%155},{200-30*i%155},1.0)\" for i in range(len(group['members']))]\n",
    "        fill_colors = [f\"rgba({100+30*i%155},{100+60*i%155},{200-30*i%155},0.2)\" for i in range(len(group['members']))]\n",
    "        for i, member in enumerate(group['members']):\n",
    "            reconstructed = reconstruct_from_group(group, member, apply_transformation)\n",
    "            reconstructed = np.array(reconstructed)\n",
    "            reconstructed_closed = np.vstack([reconstructed, reconstructed[0]])\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=reconstructed_closed[:, 0], y=reconstructed_closed[:, 1],\n",
    "                mode='lines',\n",
    "                name=f\"Original {member['original_index']}\",\n",
    "                line=dict(width=2, color=colors[i % len(colors)]),\n",
    "                marker=dict(size=6, color=colors[i % len(colors)]),\n",
    "                fill='toself',\n",
    "                fillcolor=colors[i % len(colors)]\n",
    "            ))\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f'Group {group_idx} ({len(group[\"members\"])} members)',\n",
    "            width=width, height=height,\n",
    "            xaxis=dict(scaleanchor=\"y\", scaleratio=1, showgrid=False, zeroline=False),\n",
    "            yaxis=dict(scaleanchor=\"x\", scaleratio=1, showgrid=False, zeroline=False),\n",
    "            showlegend=False,\n",
    "            plot_bgcolor=\"#FFF\",\n",
    "        )\n",
    "        fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bec4fd1-eed5-4ca4-9cde-0f2e101427bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_polygon_groups(groups, apply_transformation, width=1300, height=1300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47155c30-6627-4f8d-9f5d-77fee81ebd29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9e48c2-bc0d-4d88-9d58-63ca80c29b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47573bcf-a213-4ca6-8025-dab3a1e72efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_polygon_groups(groups, apply_transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d8953-2ae2-43bf-9247-540797931af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfcd7a7-7dcb-4a9d-a78f-5847880ca297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4465248-c435-4fc5-86b2-caaa0e24d028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b39382e-4721-4b46-8cb5-3ee9d420124a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2aa6cd-1cdf-4abf-a8ce-5a1c8db43487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4ff0ab-0110-49d8-b16b-a3aa4f7b12fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e5d2e0-012b-460a-bd45-2db7edbd0fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea91e25-1b5d-4d51-a155-6da67693e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = gf.c.straight_heater_doped_rib(length=100)\n",
    "layer_stack = get_layer_stack()\n",
    "j = to_json(c, layer_stack=layer_stack, return_json=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d79cb6c-b03f-495e-835d-4ef9c7d073bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons = [part for part in j[\"parts\"] if part[\"name\"] == \"VIA1\"][0][\"shape\"][\"polygons\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75df0f3b-fa6f-4291-9f60-62bac40404a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def group_translation_congruent_polygons(polygons):\n",
    "    # Round coordinates and convert to uniform float32 dtype\n",
    "    polygons_rounded = [np.round(p.astype(np.float32), 4) for p in polygons]\n",
    "\n",
    "    # Calculate centroids using list comprehension\n",
    "    centroids = np.array([np.mean(p, axis=0) for p in polygons_rounded])\n",
    "\n",
    "    # Normalize using broadcasting-friendly format\n",
    "    normalized = [p - c for p, c in zip(polygons_rounded, centroids)]\n",
    "\n",
    "    # Create hashable keys with length encoding\n",
    "    norm_keys = [tuple([len(p)] + p.ravel().round(3).tolist()) for p in normalized]\n",
    "\n",
    "    # Group with dictionary comprehension\n",
    "    groups = {}\n",
    "    for idx, key in enumerate(norm_keys):\n",
    "        if key not in groups:\n",
    "            groups[key] = {\n",
    "                \"polygon\": normalized[idx].ravel(),\n",
    "                \"offsets\": [],\n",
    "            }\n",
    "        groups[key][\"offsets\"].extend(centroids[idx].tolist())\n",
    "    for key in groups.keys():\n",
    "        groups[key][\"offsets\"] = np.asarray(groups[key][\"offsets\"], dtype=\"float32\")\n",
    "    return list(groups.values())\n",
    "\n",
    "groups = group_translation_congruent_polygons(polygons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850265f6-b5d4-4624-b595-e4881a47dd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c901a33-2257-4bcc-bf4e-9baef922b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups[0][\"polygon\"].reshape(-1,2) + groups[0][\"offsets\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609ba68f-3585-4e41-8370-a8df5f070de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.to_3d().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ea38c3-a8ba-452f-85c9-7101cf96d6ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
